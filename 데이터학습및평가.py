# -*- coding: utf-8 -*-
# === 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (Phase 5 - Mac í˜¸í™˜ ë²„ì „) ===
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor 

# ğŸŒŸ [ì¤‘ìš”] Mac í•œê¸€ í°íŠ¸ ê¹¨ì§ ë°©ì§€ ì„¤ì • ğŸŒŸ
# Macì—ì„œëŠ” 'AppleGothic'ì„ ì‚¬ìš©í•´ì•¼ ê·¸ë˜í”„ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
plt.rcParams['font.family'] = 'AppleGothic' 
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤(-) ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

INPUT_FILE = 'apartment_rent_final_features.csv'

def train_and_evaluate():
    print("--- ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì‹œì‘ ---")
    
    # 1. ë°ì´í„° ë¡œë“œ
    if not os.path.exists(INPUT_FILE):
        print(f"[ì˜¤ë¥˜] '{INPUT_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Phase 4(ë°ì´í„°ë¶„ì„œ.py)ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ğŸŒŸ [ì¤‘ìš”] CSV í•œê¸€ ê¹¨ì§ ë°©ì§€: encoding='utf-8-sig' ì‚¬ìš©
    try:
        df = pd.read_csv(INPUT_FILE, encoding='utf-8-sig')
    except UnicodeDecodeError:
        # í˜¹ì‹œ utf-8-sigë¡œ ì•ˆ ì—´ë¦¬ë©´ cp949ë¡œ ì‹œë„ (ë¹„ìƒìš©)
        df = pd.read_csv(INPUT_FILE, encoding='cp949')
        
    print(f"ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(df)}ê±´")

    # 2. í•™ìŠµ ë°ì´í„°(X)ì™€ íƒ€ê²Ÿ(y) ë¶„ë¦¬
    target_col = 'log_ë³´ì¦ê¸ˆ'
    drop_cols = ['log_ë³´ì¦ê¸ˆ', 'ì „ì„¸í™˜ì‚°ê°€(ë§Œì›)'] 
    
    # ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚­ì œ (ì•ˆì „ì¥ì¹˜)
    available_drop_cols = [col for col in drop_cols if col in df.columns]
    X = df.drop(columns=available_drop_cols)
    y = df[target_col]
    
    print(f"\ní•™ìŠµì— ì‚¬ìš©í•  íŠ¹ì„±(Features): {list(X.columns)}")

    # 3. ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ: í•™ìŠµìš© {len(X_train)}ê±´, í…ŒìŠ¤íŠ¸ìš© {len(X_test)}ê±´")

    # 4. ëª¨ë¸ í•™ìŠµ (Random Forest)
    print("\n[ëª¨ë¸ í•™ìŠµ ì¤‘...] Random Forest ì•Œê³ ë¦¬ì¦˜ì´ íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤.")
    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)
    print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

    # 5. ì˜ˆì¸¡ ìˆ˜í–‰
    y_pred_log = model.predict(X_test)

    # 6. ê²°ê³¼ ì—­ë³€í™˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼ -> ì›ë˜ ê°€ê²© ë‹¨ìœ„)
    y_test_origin = np.expm1(y_test)
    y_pred_origin = np.expm1(y_pred_log)

    # 7. ì„±ëŠ¥ í‰ê°€
    r2 = r2_score(y_test_origin, y_pred_origin)
    rmse = np.sqrt(mean_squared_error(y_test_origin, y_pred_origin))
    mae = mean_absolute_error(y_test_origin, y_pred_origin)

    print("\n" + "="*40)
    print(" ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
    print("="*40)
    print(f" 1. ê²°ì • ê³„ìˆ˜ (R2 Score): {r2:.4f}")
    print(f" 2. í‰ê·  ì˜¤ì°¨ (RMSE): {rmse:,.0f} ë§Œì›")
    print(f" 3. í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {mae:,.0f} ë§Œì›")
    print("-" * 40)
    print(f" í•´ì„: ì´ ëª¨ë¸ì€ ì‹¤ì œ ê°€ê²©ê³¼ í‰ê· ì ìœ¼ë¡œ ì•½ {mae:,.0f}ë§Œì› ì •ë„ì˜ ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤.")
    print("="*40)

    # 8. ì‹œê°í™”
    # (1) ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‚°ì ë„
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test_origin, y=y_pred_origin, alpha=0.6, color='blue')
    plt.plot([y_test_origin.min(), y_test_origin.max()], 
             [y_test_origin.min(), y_test_origin.max()], 
             'r--', lw=2)
    plt.title('ì‹¤ì œ ê°€ê²© vs ì˜ˆì¸¡ ê°€ê²© (Actual vs Predicted)') # í•œê¸€ ì œëª©
    plt.xlabel('ì‹¤ì œ ê°€ê²© (ì›)')
    plt.ylabel('ì˜ˆì¸¡ ê°€ê²© (ì›)')
    plt.grid(True)
    plt.show()

    # (2) íŠ¹ì„± ì¤‘ìš”ë„ (Feature Importance)
    plt.figure(figsize=(10, 6))
    sorted_idx = model.feature_importances_.argsort()
    plt.barh(X.columns[sorted_idx], model.feature_importances_[sorted_idx], color='green')
    plt.title('ë³€ìˆ˜ ì¤‘ìš”ë„ (Feature Importance)') # í•œê¸€ ì œëª©
    plt.xlabel('ì¤‘ìš”ë„')
    plt.grid(axis='x')
    plt.show()

if __name__ == "__main__":
    train_and_evaluate()